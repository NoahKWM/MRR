---
title: "MRR Project"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-11-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()); graphics.off()
```

# Introduction

This project is about the diagnosis of breast cancer. Thanks to the dataset "Breast Cancer Wisconsin (Diagnostic) Data Set" from the computer sciences department at the University of Wisconsin, we will try to predict if a tumor is benign or malignant.

### Imports
```{r, message=FALSE}
library("ggplot2")
library("MASS")
library("reshape2")
library("corrplot")
library("caret")
data <- read.csv('data.csv', header = TRUE)
```


### Data exploration
Let's take a look at all the missing values of the dataset, if there's any, and clean the dataset:
```{r, echo=FALSE}
data <- data[, -ncol(data)]
data <- data[, -1]
missing_values <- colSums(is.na(data))
print(missing_values)
```

There's no missing values in this dataset.
So now, we have 569 observations and 30 covariables to predict 1 target variable, which is the diagnosis.

Now, let's explain the covariables: 

There are ten real-valued features computed for each cell nucleus:

  a) radius (mean of distances from center to points on the perimeter)
  b) texture (standard deviation of gray-scale values)
  c) perimeter
  d) area
  e) smoothness (local variation in radius lengths)
  f) compactness (perimeter^2 / area - 1.0)
  g) concavity (severity of concave portions of the contour)
  h) concave points (number of concave portions of the contour)
  i) symmetry
  j) fractal dimension ("coastline approximation" - 1)

Then, the mean, the standard error and the "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.


```{r, echo=FALSE}
X <- data[-1]
y <- data[1]
n <- nrow(data)
p <- ncol(data)
```



# Description of the dataset


### Data visualization

Let's now visualize the data to see if there's any outliers or if the data is normally distributed.

```{r, echo=FALSE}

var_mean <- c("diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave.points_mean", "symmetry_mean", "fractal_dimension_mean")
data_mean <- data[, var_mean]

var_se <- c("diagnosis", "radius_se", "texture_se","perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave.points_se", "symmetry_se", "fractal_dimension_se" )
data_se <- data[, var_se]

var_worst <- c("diagnosis", "radius_worst", "texture_worst","perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave.points_worst", "symmetry_worst", "fractal_dimension_worst" )
data_worst <- data[, var_worst]
```

```{r, echo=FALSE}
ggplot(data = melt(data_mean, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x")

ggplot(data = melt(data_se, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x")

ggplot(data = melt(data_worst, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x")
```


It looks like most of the features are normally distributed, and there seems to be no outliers.
However, there's no clear separation between benign and malignant for most of the features except "concave.points_worst", "perimeter_worst", "radius_worst" and "concave.points_mean".
So a visualization of the values of these features is not enough.


Let's now look at the coorelation plot :

```{r, echo=FALSE}
corr_mat <- cor(data[, -1])
corrplot(corr_mat, order = "hclust", tl.cex = 0.7) 
```

We can see that there's a strong correlation between some features, which can be a problem for the model.


We can now try 3 diffents approaches, and we will apply the same methods on each of them in the future:
- No covariables deletion
- Covariables deletion by highest correlation
- Covariables deletion by PCA


#### No covariables deletion

We still use the same dataset.


#### Covariables deletion by highest correlation

If two covariables are highly correlated, (for instance, if the correlation is higher than 0.8), we will delete the one that has the highest mean of correlations with the others.
We can use the function "findCorrelation" from the package "caret" to provide us the said covariables that we will delete.

```{r, echo=FALSE}
corr_var <- colnames(data)[findCorrelation(x = corr_mat, cutoff = 0.8, verbose = TRUE)]
corr_var
```

```{r, echo=FALSE}
covar_clear <- data[, which(!colnames(data) %in% corr_var)]
```

Then, we only keep the covariables that are not in the list "corr_var".
```{r, echo=FALSE}
data_clear <- cbind(diagnosis = data$diagnosis, covar_clear)
print(colnames(data_clear))
```

We now have a new dataset with less covariables, only 16.
However, this approch is not perfect, because we only scan the correlation between two covariables once:
there could still be high correlations between variables, and deleting more covariables could reduce to much information.


#### Covariables deletion by PCA

We will now use the PCA method to reduce the number of covariables.
We will use the function "prcomp" from the package "stats" to do so.
Because each covariable has a different scale, we will use the parameters "center = TRUE" and "scale. = TRUE" to standardize the covariables.
```{r, echo=FALSE}
pca <- prcomp(data[, -1], center = TRUE, scale. = TRUE)
plot(pca, type = "l")
summary(pca)
```

Let's arbitrarily choose to keep enough covariables to explain at least 99% of the variance.
According to the summary, we need to keep 17 covariables to do so (99.11% of the variance explained).

```{r, echo=FALSE}
pca <- prcomp(data[, -1], center = TRUE, scale. = TRUE, rank. = 17)
data_pca <- as.data.frame(pca$x)
```

Though, 17 covariables means 17 dimensions, which is too much to visualize.
So we can also choose to keep only 2 covariables, which will allow us to visualize the data.
Together, they explain 63.24% of the variance.
  
```{r, echo=FALSE}
ggplot(data_pca, aes(x=PC1, y=PC2, col=data$diagnosis)) + geom_point(alpha=0.5)
```

Visually, we can see that there's a clear separation between benign and malignant tumors.
Let's draw their respective distributions:
  
```{r, echo=FALSE}
ggplot(data_pca, aes(x=PC1, fill=data$diagnosis)) + geom_density(alpha=0.5)
ggplot(data_pca, aes(x=PC2, fill=data$diagnosis)) + geom_density(alpha=0.5)
```

With the first PC1, we can see that the means of the distributions are far enough from each other to be able to separate them.
However, with the second PC2, the means are too close to each other, so we can't separate them with this principal component.
And given that the following principal components explain even less variance, we can't use them to separate the distributions either.