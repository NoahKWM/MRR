---
title: "MRR Project"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-11-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()); graphics.off()
```

# Introduction

This project is about the diagnosis of breast cancer. Thanks to the dataset "Breast Cancer Wisconsin (Diagnostic) Data Set" from the computer sciences department at the University of Wisconsin, we will try to predict if a tumor is benign or malignant.

```{r, message=FALSE, echo=FALSE}
library("ggplot2")
library("MASS")
library("reshape2")
library("corrplot")
library("caret")
library("glmnet")
data <- read.csv('data.csv', header = TRUE)
```


## Preprocessing and Data Exploration
Let's take a look at all the missing values of the dataset, if there's any, and clean the dataset:
```{r, echo=FALSE}
data <- data[, -ncol(data)]
data <- data[, -1]
```

There's no missing values in this dataset, but we have to drop the last unnamed column and the "id" one which is not usefull.
So now, we have 569 observations and 30 covariables to predict 1 target variable, which is the diagnosis.

Now, let's explain the covariables: 

There are ten real-valued features computed for each cell nucleus:

  a) radius (mean of distances from center to points on the perimeter)
  b) texture (standard deviation of gray-scale values)
  c) perimeter
  d) area
  e) smoothness (local variation in radius lengths)
  f) compactness (perimeter^2 / area - 1.0)
  g) concavity (severity of concave portions of the contour)
  h) concave points (number of concave portions of the contour)
  i) symmetry
  j) fractal dimension ("coastline approximation" - 1)

Then, the mean, the standard error and the "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.


```{r, echo=FALSE}
X <- data[-1]
y <- data[1]
n <- nrow(data)
p <- ncol(data)
```



# Description of the dataset


## Data visualization

Let's now visualize the data to see if there's any outliers or if the data is normally distributed.
These histograms are repetitive, so we will only show the means of the 10 features. The others are still available in the code for the reader to see.
```{r, echo=FALSE}

var_mean <- c("diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave.points_mean", "symmetry_mean", "fractal_dimension_mean")
data_mean <- data[, var_mean]

# var_se <- c("diagnosis", "radius_se", "texture_se","perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave.points_se", "symmetry_se", "fractal_dimension_se" )
# data_se <- data[, var_se]

# var_worst <- c("diagnosis", "radius_worst", "texture_worst","perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave.points_worst", "symmetry_worst", "fractal_dimension_worst" )
# data_worst <- data[, var_worst]
```

```{r, echo=FALSE}
ggplot(data = melt(data_mean, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x") + 
  theme(plot.margin = unit(c(1, 0, 1, 0), "cm"))

# ggplot(data = melt(data_se, id.var = "diagnosis"), aes(value)) +
#   geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
#   facet_wrap(~variable, scales = "free_x")

# ggplot(data = melt(data_worst, id.var = "diagnosis"), aes(value)) +
#   geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
#   facet_wrap(~variable, scales = "free_x")
```


It looks like most of the features are normally distributed, and there seems to be no outliers.
However, there's no clear separation between benign and malignant for most of the features except "concave.points_worst", "perimeter_worst", "radius_worst" and "concave.points_mean".
So a visualization of the values of these features is not enough.


Let's now look at the coorelation plot :

```{r, echo=FALSE}
corr_mat <- cor(data[, -1])
corrplot(corr_mat, order = "hclust", tl.cex = 0.7) 
```

We can see that there's a strong correlation between some features, which can be a problem for the model.


We can now try 3 diffents approaches, and we will apply the same methods on each of them in the future:
  
  - No covariables deletion
  - Covariables deletion by highest correlation
  - Covariables deletion by PCA


### No covariables deletion

We still use the same dataset.


### Covariables deletion by highest correlation

If two covariables are highly correlated, (for instance, if the correlation is higher than 0.8), we will delete the one that has the highest mean of correlations with the others.
We can use the function "findCorrelation" from the package "caret" to provide us the said covariables that we will delete.

```{r, echo=FALSE}
corr_var <- colnames(data)[findCorrelation(x = corr_mat, cutoff = 0.8, verbose = FALSE)]
```

```{r, echo=FALSE}
covar_clear <- data[, which(!colnames(data) %in% corr_var)]
```

Then, we only keep the covariables that are not in the list "corr_var", which gives us:
```{r, echo=FALSE}
data_clear <- cbind(diagnosis = data$diagnosis, covar_clear)
print(colnames(data_clear))
```

We now have a new dataset with less covariables, only 16.
However, this approch is not perfect, because we only scan the correlation between two covariables once:
there could still be high correlations between variables if 3 or more covariables were correlated together, and deleting more covariables could reduce to much information.


### Covariables deletion by PCA

We will now use the PCA method to reduce the number of covariables.
We will use the function "prcomp" from the package "stats" to do so.
Because each covariable has a different scale, we will use the parameters "center = TRUE" and "scale. = TRUE" to standardize the covariables.
```{r, echo=FALSE}
pca <- prcomp(data[, -1], center = TRUE, scale. = TRUE)
plot(pca, type = "l")
summary(pca)
```

Let's arbitrarily choose to keep enough covariables to explain at least 99% of the variance.
According to the summary, we need to keep 17 covariables to do so (99.11% of the variance explained).

```{r, echo=FALSE}
pca <- prcomp(data[, -1], center = TRUE, scale. = TRUE, rank. = 17)
data_pca <- cbind(as.data.frame(pca$x), diagnosis = data$diagnosis)
```

Though, 17 covariables means 17 dimensions, which is too much to visualize.
So we can temporarily choose to keep 2 covariables, which will allow us to visualize the data.
Together, they explain 63.24% of the variance.
  
```{r, echo=FALSE}
ggplot(data_pca, aes(x=PC1, y=PC2, col=data$diagnosis)) + geom_point(alpha=0.5)
```

Visually, we can see that there's a clear separation between benign and malignant tumors, but only thanks to the first principal component:
If we guess the center of the two clusters of diagnosis, we can see that their PC1 values are far enough from each other to be able to separate them (around -3 for the malignant tumors and 2.5 for the benign ones),
but their PC2 values are too close to each other (around 0), so we can't separate them with this principal component.
Let's draw only the first distribution on PC1 as the PC2 one is not useful (the reader can still see the PC2 distribution in the code):

```{r, echo=FALSE}
ggplot(data_pca, aes(x=PC1, fill=data$diagnosis)) + geom_density(alpha=0.5)
# ggplot(data_pca, aes(x=PC2, fill=data$diagnosis)) + geom_density(alpha=0.5)
```

With the first PC1, we can see that the means of the distributions are far enough from each other to be able to separate them.
But the PC1 only explains 44.27% of the variance, so it doesn't really give us a lot of information.
Let's keep the 17 covariables for the next steps.


### Backward selection

We will now use the backward selection method to delete the covariables that are not useful for the model.

```{r, echo=FALSE}
data_back <- data
data_back$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)
res <- glm(diagnosis ~ . , data = data_back, family = "binomial")

resback <- step(res, direction = "backward", trace = 0)
print(resback)

selected_vars <- colnames(model.matrix(resback))[!is.na(coef(resback))][-1]

# Mettre Ã  jour data_back pour ne conserver que les covariables non nulles
data_back <- data_back[, c("diagnosis", selected_vars)]

```













---

### Ridge on total dataset

```{r}
y <- ifelse(y == "M", 1, 0)
X <- as.matrix(X)
```

```{r}
lambdas <- 10^seq(-4, -1.5, 0.01)
cv_model_ridge <- cv.glmnet(X, y, family= "binomial", alpha= 0, lambda= lambdas, standardize= TRUE)
best_lambda_ridge <- cv_model_ridge$lambda.min
plot(cv_model_ridge)
```

```{r}
data_ridge <- data
data_ridge$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)
ind <- sample(n)
tab_ridge <- data_ridge[ind, ]
K <- 5
lblock <- trunc(n/K)
predictions_ridge <- c()
true_values_ridge <- tab_ridge$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk_ridge <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk_ridge <- ((k-1) * lblock + 1):n;
  }
  tabTrain_ridge <- tab_ridge[-indk_ridge, ];
  tmp_y <- tabTrain_ridge$diagnosis;
  tmp_X <- tabTrain_ridge[-1]; 
  modTrain_ridge <- glmnet(tmp_X, tmp_y, family = "binomial", alpha = 0, lambda = best_lambda_ridge);
  testk_ridge <- predict(modTrain_ridge, newx = as.matrix(tab_ridge[indk_ridge, ][-1]), type = "response", s= best_lambda_ridge);
  predk_ridge <- ifelse(testk_ridge > 0.5, 1, 0);
  predictions_ridge <- c(predictions_ridge, predk_ridge);
}

table(true_values_ridge, predictions_ridge)
print(paste("Accuracy: ", round(mean(predictions_ridge == true_values_ridge), 2)))
```


### Covariables deletion by Backward selection


```{r}
ind <- sample(n)
tab_back <- data_back[ind, ]
K <- 5
lblock <- trunc(n/K)
predictions_back <- c()
true_values_back <- tab_back$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_back <- tab_back[-indk, ];
  modTrain_back <- glm(diagnosis ~ . , data = tabTrain_back, family = "binomial");
  testk_back <- predict.glm(modTrain_back, newdata = tab_back[indk, ][-1], type = "response");
  predk_back <- ifelse(testk_back > 0.5, 1, 0);
  predictions_back <- c(predictions_back, predk_back);
}

table(true_values_back, predictions_back)
print(paste("Accuracy: ", round(mean(predictions_back == true_values_back), 2)))
```






















### Covariables deletion by highest correlation

```{r}
data_clear$diagnosis <- ifelse(data_clear$diagnosis == "M", 1, 0)
```

```{r}
ind <- sample(n)
tab_clear <- data_clear[ind, ]
K <- 5
lblock <- trunc(n/K)
predictions_clear <- c()
true_values_clear <- tab_clear$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_clear <- tab_clear[-indk, ];
  modTrain_clear <- glm(diagnosis ~ . , data = tabTrain_clear, family = "binomial");
  testk_clear <- predict.glm(modTrain_clear, newdata = tab_clear[indk, ][-1], type = "response");
  predk_clear <- ifelse(testk_clear > 0.5, 1, 0);
  predictions_clear <- c(predictions_clear, predk_clear);
}

table(true_values_clear, predictions_clear)
print(paste("Accuracy: ", round(mean(predictions_clear == true_values_clear), 2)))
```



### Covariables deletion by PCA

```{r}
data_pca$diagnosis <- ifelse(data_pca$diagnosis == "M", 1, 0)
```

```{r}
ind <- sample(n)
tab_pca <- data_pca[ind, ]
K <- 5
lblock <- trunc(n/K)
predictions_pca <- c()
true_values_pca <- tab_pca$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_pca <- tab_pca[-indk, ];
  modTrain_pca <- glm(diagnosis ~ . , data = tabTrain_pca, family = "binomial");
  testk_pca <- predict.glm(modTrain_pca, newdata = tab_pca[indk, ], type = "response");
  predk_pca <- ifelse(testk_pca > 0.5, 1, 0);
  predictions_pca <- c(predictions_pca, predk_pca);
}

table(true_values_pca, predictions_pca)
print(paste("Accuracy: ", round(mean(predictions_pca == true_values_pca), 2)))
```

On remarque que la mÃ©thode de Ridge est la plus prÃ©cise, avec une accuracy de 0.98.
En seconde position, on retrouve la mÃ©thode de PCA, avec une accuracy de 0.97.
Enfin, on retrouve les mÃ©thodes de backward selection et de suppression des covariables par corrÃ©lation, avec une accuracy de 0.96.

