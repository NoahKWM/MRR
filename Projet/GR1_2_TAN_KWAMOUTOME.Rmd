---
title: "MRR Project"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-11-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()); graphics.off()
```

# Introduction

Nowadays, in the realm of healthcare and data analytic, diagnosis of breast cancer has become a crucial task: In
2020, 685 000 deaths due to breast cancer were recorded according to the World Health Organization. Though,
roughly half of breast cancers occur in women without any specific risk factors other than sex and age. It means
that the diagnosis and prediction of malignant tumors is of the utmost importance, as it enables cancer patients
to be treated as soon and effectively as possible or even prevent risk.
This project is about the diagnosis and prediction of breast cancer. Thanks to the dataset “Breast Cancer Wisconsin
(Diagnostic) Data Set” from the computer sciences department at the University of Wisconsin, we will try to develop
predictive models capable to predict if a tumor is benign or malignant given pictures of tumors’ cells.

```{r, message=FALSE}
library("ggplot2")
library("MASS")
library("reshape2")
library("corrplot")
library("caret")
library("glmnet")
data <- read.csv('data.csv', header = TRUE)
```


# Preprocessing and Data Exploration

Let's take a look at all the missing values of the dataset, if there's any, and clean the dataset:
```{r, echo=FALSE}
data <- data[, -ncol(data)]
data <- data[, -1]
missing_values <- sum(colSums(is.na(data)))
print(paste("Number of missing values: ", missing_values))
```

There's no missing values in this dataset, but we have to drop the last unnamed column and the "id" one which is not usefull.
So now, we have 569 observations and 30 covariables to predict 1 target variable, which is the diagnosis.

Now, let's explain the covariables: 

There are ten real-valued features computed for each cell nucleus:

  a) radius (mean of distances from center to points on the perimeter)
  b) texture (standard deviation of gray-scale values)
  c) perimeter
  d) area
  e) smoothness (local variation in radius lengths)
  f) compactness (perimeter^2 / area - 1.0)
  g) concavity (severity of concave portions of the contour)
  h) concave points (number of concave portions of the contour)
  i) symmetry
  j) fractal dimension ("coastline approximation" - 1)

Then, the mean, the standard error and the "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.
Furthermore, there are 212 Malignant diagnoses and 357 Benign diagnoses, which is a relatively balanced split: we
do have enough data from each diagnoses to study the dataset correctly.


```{r, echo=FALSE}
n <- nrow(data)
p <- ncol(data)
data$diagnosis <- as.factor(ifelse(data$diagnosis == "M", 1, 0))
```



# Description of the dataset


## Data visualization

Let's now visualize the data to see if there's any outliers or if the data is normally distributed.
These histograms are repetitive, so we will only show the means of the 10 features.
```{r, echo=FALSE}

var_mean <- c("diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave.points_mean", "symmetry_mean", "fractal_dimension_mean")
data_mean <- data[, var_mean]

var_se <- c("diagnosis", "radius_se", "texture_se","perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave.points_se", "symmetry_se", "fractal_dimension_se" )
data_se <- data[, var_se]

var_worst <- c("diagnosis", "radius_worst", "texture_worst","perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave.points_worst", "symmetry_worst", "fractal_dimension_worst" )
data_worst <- data[, var_worst]
```

```{r, echo=FALSE}
ggplot(data = melt(data_mean, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x") + 
  theme(plot.margin = unit(c(1, 0, 1, 0), "cm"))

ggplot(data = melt(data_se, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x")

ggplot(data = melt(data_worst, id.var = "diagnosis"), aes(value)) +
  geom_histogram(bins = 20, aes(fill=diagnosis), alpha = 0.5) +
  facet_wrap(~variable, scales = "free_x")
```


It looks like most of the features are normally distributed, and there seems to be no outliers.
However, there's no clear separation between benign and malignant for most of the features except "concave.points_worst", "perimeter_worst", "radius_worst" and "concave.points_mean".
So a visualization of the values of these features is not enough.

Also, we can see that the scale of the features is not the same, which could be a problem for the model.
We will use the function "scale" to standardize the covariables.

```{r, echo=FALSE}
data[, -1] <- scale(data[, -1])
```

Let's now look at the coorelation plot :

```{r, echo=FALSE}
corr_mat <- cor(data)
corrplot(corr_mat, order = "hclust", tl.cex = 0.7)
data$diagnosis <- as.factor(data$diagnosis) 
```

We can see that there's a strong correlation between some features, which can be a problem for the model.

However, there is a significant correlation between diagnosis and the following variables: diagnosis, radius mean,
perimeter mean, area mean, concave.points mean, radius worst, perimeter worst, area worst, concave.points worst.
That could mean that these co-variables may be important for the prediction of the target variable.

We will try 5 approaches to study the dataset:
- No treatment
- Ridge penalization
- Backward Selection
- Co-variables deletion by highest correlation
- Co-variables deletion by PCA

We will use a k-fold method to build a model training with the 4 approaches, and predict the diagnosis. We will use K = 5, meaning that 20% of the 569 observations will compose the test set and 80% will compose the training set.

```{r, echo=FALSE}
ind <- sample(n)
K <- 5
lblock <- trunc(n/K)
```

### No Treatment

We still use the same dataset and use the glm function.

```{r, echo=FALSE, warning=FALSE}
tab <- data[ind, ]
predictions <- c()
true_values <- tab$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain <- tab[-indk, ];
  modTrain <- glm(diagnosis ~ . , data = tabTrain, family = "binomial", maxit= 10000);
  testk <- predict.glm(modTrain, newdata = tab[indk, ][-1], type = "response");
  predk <- ifelse(testk > 0.5, 1, 0);
  predictions <- c(predictions, predk);
}

table(true_values, predictions)
print(paste("Accuracy: ", round(mean(predictions == true_values), 3)))
```

Without any treatment of the dataset, we get a 0.95 accuracy, which is already a good result. But let's see if we can improve it.



### Ridge on total dataset

```{r, echo=FALSE}
y <- data$diagnosis
X <- as.matrix(data[, -1])
```

```{r, echo=FALSE, warning=FALSE}
lambdas <- 10^seq(-4, -1.5, 0.01)
cv_model_ridge <- cv.glmnet(X, y, family= "binomial", alpha= 0, lambda= lambdas, standardize= TRUE)
best_lambda_ridge <- cv_model_ridge$lambda.min
plot(cv_model_ridge)
```

```{r, echo=FALSE, warning=FALSE}
data_ridge <- data
tab_ridge <- data_ridge[ind, ]
predictions_ridge <- c()
true_values_ridge <- tab_ridge$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_ridge <- tab_ridge[-indk, ];
  tmp_y <- tabTrain_ridge$diagnosis;
  tmp_X <- tabTrain_ridge[-1]; 
  modTrain_ridge <- glmnet(tmp_X, tmp_y, family = "binomial", alpha = 0, lambda = best_lambda_ridge);
  testk_ridge <- predict(modTrain_ridge, newx = as.matrix(tab_ridge[indk, ][-1]), type = "response", s= best_lambda_ridge);
  predk_ridge <- ifelse(testk_ridge > 0.5, 1, 0);
  predictions_ridge <- c(predictions_ridge, predk_ridge);
}

table(true_values_ridge, predictions_ridge)
print(paste("Accuracy: ", round(mean(predictions_ridge == true_values_ridge), 3)))
```

Without deleting any covariables, we get a 0.98 accuracy, which is a better result than the previous one.
The Ridge method is a penalization method that shrinks the coefficients of the covariables that are not useful for the model.
It reduces overfitting and may improve the accuracy of the model when covariables are correlated, which is the case here.

But now, let's see if we can get a better accuracy by deleting some covariables.

### Backward selection

We will now use the backward selection method to delete the covariables that are not useful for the model with
the function ”step”.
With this method, the co-variables left are:

```{r, echo=FALSE, warning=FALSE}
data_back <- data
res <- glm(diagnosis ~ . , data = data_back, family = "binomial")

resback <- step(res, direction = "backward", trace = 0)
print(resback)

selected_vars <- colnames(model.matrix(resback))[!is.na(coef(resback))][-1]

data_back <- data_back[, c("diagnosis", selected_vars)]

```
So we get a total of 24 co-variables, the step method only deleted 6 co-variables, which isn’t really effective.



```{r, echo=FALSE, warning=FALSE}
tab_back <- data_back[ind, ]
predictions_back <- c()
true_values_back <- tab_back$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_back <- tab_back[-indk, ];
  modTrain_back <- glm(diagnosis ~ . , data = tabTrain_back, family = "binomial", maxit= 10000);
  testk_back <- predict.glm(modTrain_back, newdata = tab_back[indk, ][-1], type = "response");
  predk_back <- ifelse(testk_back > 0.5, 1, 0);
  predictions_back <- c(predictions_back, predk_back);
}

table(true_values_back, predictions_back)
print(paste("Accuracy: ", round(mean(predictions_back == true_values_back), 3)))
```

We can see that the accuracy is only slightly better than the default one with no treatment. So the backward method did improve the model, but not by a lot.
Also, the model is less accurate than the Ridge one, so it should not be privileged over the Ridge method.


### Covariables deletion by highest correlation

If two covariables are highly correlated, (for instance, if the correlation is higher than 0.8), we will delete the one that has the highest mean of correlations with the others.
We can use the function "findCorrelation" from the package "caret" to provide us the said covariables that we will delete.
```{r, echo=FALSE}
corr_mat_tmp <- cor(data[, -1])
corr_var <- colnames(data)[findCorrelation(x = corr_mat_tmp, cutoff = 0.8, verbose = FALSE)]
```

```{r, echo=FALSE}
covar_clear <- data[, which(!colnames(data) %in% corr_var)]
```

Then, we only keep the covariables that are not in the list "corr_var", which gives us:
```{r, echo=FALSE}
data_clear <- cbind(diagnosis = data$diagnosis, covar_clear)
print(colnames(data_clear))
```

We now have a new dataset with less covariables, only 15, which is half of the initial variables. However, this approch is not perfect,
because we only scan the correlation between two covariables once: there could still be high correlations between
variables if 3 or more covariables were correlated together, and deleting more covariables could reduce too much
information.


```{r, echo=FALSE, warning=FALSE}
tab_clear <- data_clear[ind, ]
predictions_clear <- c()
true_values_clear <- tab_clear$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_clear <- tab_clear[-indk, ];
  modTrain_clear <- glm(diagnosis ~ . , data = tabTrain_clear, family = "binomial", maxit = 10000);
  testk_clear <- predict.glm(modTrain_clear, newdata = tab_clear[indk, ][-1], type = "response");
  predk_clear <- ifelse(testk_clear > 0.5, 1, 0);
  predictions_clear <- c(predictions_clear, predk_clear);
}

table(true_values_clear, predictions_clear)
print(paste("Accuracy: ", round(mean(predictions_clear == true_values_clear), 3)))
```

With this method, we get a better accuracy than the default one.
However, the accuracy is globally as good as the one with the backward selection method.
Still, it is less accurate than the Ridge method, so it should not be privileged over the Ridge method.


### Covariables deletion by PCA

We will now use the PCA method to reduce the number of covariables.
We will use the function "prcomp" from the package "stats" to do so.
Because each covariable has a different scale, we will use the parameters "center = TRUE" and "scale. = TRUE" to standardize the covariables.
```{r, echo=FALSE}
pca <- prcomp(data[, -1], center = FALSE, scale. = FALSE)
plot(pca, type = "l")
summary(pca)
```

Let's arbitrarily choose to keep enough covariables to explain at least 99% of the variance.
According to the summary, we need to keep 17 covariables to do so (99.11% of the variance explained).

```{r, echo=FALSE}
pca <- prcomp(data[, -1], center = FALSE, scale. = FALSE, rank. = 17)
data_pca <- cbind(as.data.frame(pca$x), diagnosis = data$diagnosis)
```

Though, 17 covariables means 17 dimensions, which is too much to visualize.
So we can temporarily choose to keep 2 covariables, which will allow us to visualize the data.
Together, they explain 63.24% of the variance.
  
```{r, echo=FALSE}
ggplot(data_pca, aes(x=PC1, y=PC2, col=data$diagnosis)) + geom_point(alpha=0.5)
```

Visually, we can see that there's a clear separation between benign and malignant tumors, but only thanks to the first principal component:
If we guess the center of the two clusters of diagnosis, we can see that their PC1 values are far enough from each other to be able to separate them (around -3 for the malignant tumors and 2.5 for the benign ones),
but their PC2 values are too close to each other (around 0), so we can't separate them with this principal component.
Let's draw only the first distribution on PC1 as the PC2 one is not useful (the reader can still see the PC2 distribution in the code):

```{r, echo=FALSE}
ggplot(data_pca, aes(x=PC1, fill=data$diagnosis)) + geom_density(alpha=0.5)
ggplot(data_pca, aes(x=PC2, fill=data$diagnosis)) + geom_density(alpha=0.5)
```

With the first PC1, we can see that the means of the distributions are far enough from each other to be able to separate them.
But the PC1 only explains 44.27% of the variance, so it doesn't really give us a lot of information.
Let's keep the 17 covariables for the next steps.


```{r, echo=FALSE, warning=FALSE}
tab_pca <- data_pca[ind, ]
predictions_pca <- c()
true_values_pca <- tab_pca$diagnosis
for (k in (1:K)) {
  if (k != K) {
    indk <- ((k-1) * lblock + 1):(k * lblock);
  }
  else {
    indk <- ((k-1) * lblock + 1):n;
  }
  tabTrain_pca <- tab_pca[-indk, ];
  modTrain_pca <- glm(diagnosis ~ . , data = tabTrain_pca, family = "binomial", maxit= 10000);
  testk_pca <- predict.glm(modTrain_pca, newdata = tab_pca[indk, ], type = "response");
  predk_pca <- ifelse(testk_pca > 0.5, 1, 0);
  predictions_pca <- c(predictions_pca, predk_pca);
}

table(true_values_pca, predictions_pca)
print(paste("Accuracy: ", round(mean(predictions_pca == true_values_pca), 3)))
```

With this method, we get a better accuracy than the default one.
The accuracy is globally better than the one with the backward selection method and the one with the covariables deletion by highest correlation method.
But, it is still less accurate than the Ridge method, so it should not be privileged over the Ridge method.


We can see that the Ridge method is the most accurate, followed by PCA by only a slight percentage. Moreover,
the Ridge method gives the lowest False Negative, which is the only value that we want to minimize: False positives
(when the prediction gives ”Malignant” when the true value is ”Benign”) are not really a issue. But False negatives
(when the prediction gives ”Benign” when the true value is ”Malignant”) are the worst case scenario, as the patient
might be in danger without our knowledge.


# Conclusion

The results of our models obtained from the breast cancer classification project seem to be really accurate overall.
The highest accuracy, Sensitivity and Specificity of the prediction on 569 observations is achieved by the Ridge
Model. It may hold promise for supporting clinical decision-making of the diagnosis of tumors given images.
On a broader scale, we can then decide to make another type of model, where we can make a greater penalization
on False negatives, which are the cases we want to avoid. It will probably reduce the accuracy of the current model,
but would be more helpful in our scenarios.