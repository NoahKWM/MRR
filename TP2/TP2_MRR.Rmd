---
title: "TP2 MRR"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-10-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()); graphics.off()
```

# IV. Cookies Study

First, let's go back to our previous results: We saw that with linear regression without penalty, only 2 explanatory variables were significant.
We can deduce that we can also use a Lasso regression to select the most important features to predict the fat value of a cookie.
Futhermore, we can use a Ridge regression to see if the model is overfitting or not.


## Imports
```{r}
cookies_data <- read.csv("cookies.csv")
```

## Features extraction

For each line (meaning, for each cookie), we will use the different spectral values to compute: the mean, the standard deviation, the slope, the minimum and the maximum. 

```{r}
# Computation (mean, standard deviation, minimum and maximum)

cookies_data$mean <- rowMeans(cookies_data[, 2:701])
cookies_data$stDev <- apply(cookies_data[, 2:701], 1, sd)
cookies_data$min <- apply(cookies_data[, 2:701], 1, min)
cookies_data$max <- apply(cookies_data[, 2:701], 1, max)


# Computation (slope)

# Function: compute_slope
# @param: spectrum_values of a cookie (here, column 2 to 701)
# @return: slope of the spectrum curve for a cookie
compute_slope <- function(spectrum_values) {
  pos <- 1:length(spectrum_values)
  lm_model <- lm(spectrum_values ~ pos)
  slope <- coef(lm_model)[2]
  return(slope)
}

cookies_data$slope <- apply(cookies_data[, 2:701], 1, compute_slope) 


# Display of the new columns
head(cookies_data[,702:706])
```

---

## Regression model

Now, we have the different features of the spectra. 

```{r}
# Only features and fat values are retrieved

cookies_features <- cookies_data[c("fat", "mean", "stDev", "slope", "min", "max")]
head(cookies_features)

X <- as.matrix(cookies_features[, -1]) # co-variables
y <- cookies_features$fat # target variable
```

```{r, echo=FALSE}
# glmnet package is required 
# install.packages("glmnet")
library(glmnet)
library(MASS)
```

### Ridge regression

We're going to do the ridge regression first, using a cross validation k-fold to choose the best value for $\lambda$.

```{r}
# Cross validation
lambdas_log <- 10^seq(-4, 1, length.out = 100)
cv_ridge <- cv.glmnet(X, y, alpha=0, lambda = (lambdas_log), standardize = TRUE)

plot(cv_ridge)
best_lambda <- cv_ridge$lambda.min # lambda that gives the lowest MSE
print(paste("The best value for lambda is", best_lambda))
```

Let's use AIC and BIC criteria to recheck this value.

```{r}
# AIC and BIC
n <- nrow(X)
p <- ncol(X)
aic <- n * log(cv_ridge$cvm) + 2 * p
bic <- n * log(cv_ridge$cvm) + log(n) * p

plot(log(cv_ridge$lambda), aic, col = "red1", type = "l", xlim = c(-8, -2), ylab = "Information Criterion")
lines(log(cv_ridge$lambda), bic, col = "blue1")
legend("bottomright", lwd = 1, col = c("red1", "blue1"), legend = c("AIC", "BIC"))

best_lambda_aic <- cv_ridge$lambda[which.min(aic)] # lambda that gives the lowest AIC
best_lambda_bic <- cv_ridge$lambda[which.min(bic)] # lambda that gives the lowest BIC
```

Now we can compare the different values for $\lambda$ we found.

```{r}
lambda_values <- c(best_lambda, best_lambda_aic, best_lambda_bic)
lambda_values
```

There are the same. We can also plot the Regularization Path.

```{r}
plot(cv_ridge$glmnet.fit, xvar = "lambda", ylim = c(-100, 100))
abline(h = 0, col = 6, lty = 3)
abline(v = log(best_lambda), col = 7, lty = 3)
legend("bottomleft", legend = c(colnames(X), "Zero", "Best Lambda"), col = 1:7, lty = 1)
```

---

Now we have the best value for $\lambda$, we do another ridge regression with this parameter and there is its results : 

```{r ,echo=FALSE}
best_model_ridge <- glmnet(X, y, alpha=0, lambda = best_lambda)
coef(best_model_ridge)
```

```{r}
predictions <- predict(best_model_ridge, newx = X)

# RMSE
rmse <- sqrt(mean((predictions - y)^2)) 
print(paste("RMSE ridge model :", rmse))

# R^2
r_squared <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)
print(paste("R^2 ridge model :", r_squared))

```

---

#### Conclusion of Ridge regression

We can see that the coefficient of the slope is very important in absolute value relatively to the others (variables are scaled). It means that the slope is a very important feature to predict the fat value of a cookie.
Futhermore, the coefficient of the mean, the standard deviation and the minimum are not null but negligible compared to the slope. It means that these features are not very important to predict the fat value of a cookie, but are more important than the max.

---

### Lasso regression

```{r}
cv_lasso <- cv.glmnet(X, y, alpha=1, lambda = (lambdas_log), standardize = TRUE)

plot(cv_lasso)
best_lambda_lasso <- cv_lasso$lambda.min # lambda that gives the lowest MSE
print(paste("The best value for lambda is", best_lambda_lasso))
```

Let's use AIC and BIC criteria to recheck this value.

```{r}
# AIC and BIC
n <- nrow(X)
p <- ncol(X)
lasso_aic <- n * log(cv_lasso$cvm) + 2 * p
lasso_bic <- n * log(cv_lasso$cvm) + log(n) * p

plot(log(cv_lasso$lambda), lasso_aic, col = "red1", type = "l", xlim = c(-8, -2), ylab = "Information Criterion")
lines(log(cv_lasso$lambda), lasso_bic, col = "blue1")
legend("bottomright", lwd = 1, col = c("red1", "blue1"), legend = c("AIC", "BIC"))

best_lambda_lasso_aic <- cv_lasso$lambda[which.min(lasso_aic)] # lambda that gives the lowest AIC
best_lambda_lasso_bic <- cv_lasso$lambda[which.min(lasso_bic)] # lambda that gives the lowest BIC
```

Now we can compare the different values for $\lambda$ we found.

```{r}
lambda_lasso_values <- c(best_lambda_lasso, best_lambda_lasso_aic, best_lambda_lasso_bic)
lambda_lasso_values
```

There are the same. We can also plot the Regularization Path.

```{r}
plot(cv_lasso$glmnet.fit, xvar = "lambda", ylim = c(-100, 100))
abline(h = 0, col = 6, lty = 3)
abline(v = log(best_lambda_lasso), col = 7, lty = 3)
legend("bottomleft", legend = c(colnames(X), "Zero", "Best Lambda"), col = 1:7, lty = 1)
```

---

Now we have the best value for $\lambda$, we do another lasso regression with this parameter and there is its results : 

```{r}
best_model_lasso <- glmnet(X, y, alpha=1, lambda = best_lambda_lasso)
coef(best_model_lasso)
```

```{r}
predictions <- predict(best_model_lasso, newx = X)

# RMSE
rmse <- sqrt(mean((predictions - y)^2)) 
print(paste("RMSE lasso model :", rmse))

# R^2
r_squared <- 1 - sum((y - predictions)^2) / sum((y - mean(y))^2)
print(paste("R^2 lasso model :", r_squared))

```

---

### Conclusion of Lasso regression

We can see that the coefficient of the slope is also very important in absolute value relatively to the others (variables are scaled). It means that the slope is a very important feature to predict the fat value of a cookie.
In this case, we can also see that the mean is not null wheras all the others are. It means that the mean could be a feature to predict the fat value of a cookie relatively to the others, but is less important than the slope.

---

## No penalization

```{r}
model_linear <- lm(y ~ X)

predictions_linear <- predict(model_linear, newdata = data.frame(X))

# RMSE
rmse_linear <- sqrt(mean((predictions_linear - y)^2)) 
print(paste("RMSE linear model :", rmse_linear))

# R^2
r_squared_linear <- 1 - sum((y - predictions_linear)^2) / sum((y - mean(y))^2)
print(paste("R^2 linear model :", r_squared_linear))
```

---

## Conclusion

We can see that for both Lasso and Ridge regressions, the RMSE is higher and the R^2 is lower than those of the linear model. 
It means that the linear model is better than the Lasso and Ridge models to predict the fat value of a cookie in our study.
There could be several reasons for that. First, the number of features is not very high, so the penalization is not very useful and we actually lost to much information. 
Secondly, the model was not overfitting.
Still, the values of the RMSE and R^2 are very close, so the difference is not very important, and those penalizations show us that the slope and mean are indead the most important features to predict the fat value of a cookie. 

---

# V2

```{r}
cookies_data <- read.csv("cookies.csv")
head(cookies_data)
```
```{r}
dim(cookies_data)
```
We see that there are 700 co-variables. We can assume that some of them are less important than the others. To see this, let's do a Ridge regression and look at the coefficient of each co-variables.

```{r}
library(glmnet)
y <- cookies_data[, 1]
X <- cookies_data[, -1]
lambda <- 10^seq(-4, 1, length.out = 1000)

cv_ridge_model <- cv.glmnet(as.matrix(X), y, alpha=0, lambda = (lambda), standardize = TRUE)
plot(cv_ridge_model)
print(cv_ridge_model)

best_lambda <- cv_ridge_model$lambda.min
best_lambda_ridge_model <- best_lambda
print(paste("Best lambda :", best_lambda))
```

```{r}	
t <- lm.ridge(fat ~ ., data = cookies_data, lambda = lambda)
plot(log(lambda), t$GCV, type = "l", col = "red1", xlab = "Lambda", ylab = "GCV")
```


Let's use AIC and BIC criteria to recheck this value.

```{r}
# AIC and BIC
n <- nrow(X)
p <- ncol(X)
ridge_model_aic <- n * log(cv_ridge_model$cvm) + 2 * p
ridge_model_bic <- n * log(cv_ridge_model$cvm) + log(n) * p

plot(log(cv_ridge_model$lambda), ridge_model_aic, col = "red1", type = "l", ylab = "Information Criterion", 
     xlim = range(log(cv_ridge_model$lambda)), ylim = c(min(ridge_model_aic, ridge_model_bic), max(ridge_model_aic, ridge_model_bic)))

lines(log(cv_ridge_model$lambda), ridge_model_bic, col = "blue1")
legend("bottomright", lwd = 1, col = c("red1", "blue1"), legend = c("AIC", "BIC"))


abline(v = log(best_lambda_ridge_model_aic), col = "red1", lty = 2)  # Vertical line for AIC
abline(v = log(best_lambda_ridge_model_bic), col = "blue1", lty = 2)

best_lambda_ridge_model_aic <- cv_ridge_model$lambda[which.min(ridge_model_aic)] # lambda that gives the lowest AIC
best_lambda_ridge_model_bic <- cv_ridge_model$lambda[which.min(ridge_model_bic)] # lambda that gives the lowest BIC
```

Now we can compare the different values for $\lambda$ we found.

```{r}
lambda_ridge_model_values <- c(best_lambda_ridge_model, best_lambda_ridge_model_aic, best_lambda_ridge_model_bic)
lambda_ridge_model_values
```

There are the same. We can also plot the Regularization Path.

```{r}
plot(cv_ridge_model$glmnet.fit, xvar = "lambda", ylim = c(-100, 100))
abline(h = 0, col = 6, lty = 3)
abline(v = log(best_lambda_ridge_model), col = 7, lty = 3)
legend("bottomleft", legend = c(colnames(X), "Zero", "Best Lambda"), col = 1:7, lty = 1)
```



```{r}
final_ridge_model <- glmnet(as.matrix(X), y, alpha=0, lambda=best_lambda)

abs_coef <- abs(coef(final_ridge_model))
```

```{r}
min(abs_coef)
paste("Number of value higher than 10^-1 : ", sum(abs_coef > 10^-1))
paste("Number of value higher than 10^-2 : ", sum(abs_coef > 10^-2))
paste("Number of value higher than 10^-3 : ", sum(abs_coef > 10^-3))
paste("Number of value higher than 10^-4 : ", sum(abs_coef > 10^-4))
```
We can see that the majority of the coefficients are lower than $10^{-1}$. Then, we could think that a lot of our co-variables are useless to predict the target variable, aka the fat.

Let's do a Lasso regression to see which co-variables are actually useful to predict the fat : 

```{r}
cv_lasso_model <- cv.glmnet(as.matrix(X), y, alpha=1)

best_lambda <- cv_lasso_model$lambda.min

best_model_lasso <- glmnet(as.matrix(X), y, lambda = best_lambda, alpha = 1)
print(best_model_lasso)
```
We can see that our model is pretty accurate (deviance of 98.09%) with only 31 co-variables used among the 700 existant.

Actually, we've shown that even less co-variables are useless than what we thought.

Now let's try to split our dataset into train and test dataset:

Ridge :
```{r}
# We split into 2 dataframe randomly
indice_train <- sample(1:nrow(cookies_data), 0.8 * nrow(cookies_data))
train_data <- cookies_data[indice_train, ]
test_data <- cookies_data[-indice_train, ]

# We define X & y for both dataframe
y_train <- train_data[, 1]
X_train <- train_data[, -1]
y_test <- test_data[, 1]
X_test <- test_data[, -1]

# We train the model with the best value for lambda
cv_ridge_model <- cv.glmnet(as.matrix(X_train), y_train, alpha = 0)
best_lambda_ridge <- cv_ridge_model$lambda.min
ridge_model <- glmnet(as.matrix(X_train), y_train, lambda = best_lambda_ridge, alpha = 0)

predictions_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = as.matrix(X_test))

error_ridge <- sqrt(mean((predictions_ridge - y_test)^2))
print(paste("RMSE Ridge :", round(error_ridge, 2)))
```


Lasso : 
```{r}
# We split into 2 dataframe randomly
indice_train <- sample(1:nrow(cookies_data), 0.8 * nrow(cookies_data))
train_data <- cookies_data[indice_train, ]
test_data <- cookies_data[-indice_train, ]

# We define X & y for both dataframe
y_train <- train_data[, 1]
X_train <- train_data[, -1]
y_test <- test_data[, 1]
X_test <- test_data[, -1]

# We train the model with the best value for lambda
cv_lasso_model <- cv.glmnet(as.matrix(X_train), y_train, alpha = 1)
best_lambda <- cv_lasso_model$lambda.min
lasso_model <- glmnet(as.matrix(X_train), y_train, lambda = best_lambda, alpha = 1)

# We make prediction on the X_test
predictions_lasso <- predict(lasso_model, s = best_lambda, newx = as.matrix(X_test))

# We compute the RMSE
error_lasso  <- sqrt(mean((predictions_lasso - y_test)^2))
print(paste("RMSE :", round(error_lasso, 2)))
```

We see that the RMSE for the Lasso regression is way better than for the Ridge one. (de rien pour l'analyse)