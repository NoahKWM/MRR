---
title: "TP2 MRR"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-10-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()); graphics.off()
```

# IV. Cookies Study

```{r}
cookies_data <- read.csv("cookies.csv")
dim(cookies_data)
```
We see that there are 700 co-variables. We can assume that some of them are less important than the others. To see this, let's do a Ridge regression and look at the coefficient of each co-variables.

```{r}
library(glmnet)
y <- cookies_data[, 1]
X <- cookies_data[, -1]

cv_ridge_model <- cv.glmnet(as.matrix(X), y, alpha=0, standardize = TRUE)
plot(cv_ridge_model, main="Ridge Regression")
print(cv_ridge_model)

best_lambda <- cv_ridge_model$lambda.min
best_lambda_ridge_model <- best_lambda
print(paste("Best lambda :", best_lambda))
```
We can also plot the Regularization Path.

```{r, echo=FALSE}
plot(cv_ridge_model$glmnet.fit, xvar = "lambda", main="Ridge Regression", xlim=c(1.5, 7))
abline(h = 0, col = 6, lty = 3)
abline(v = log(best_lambda_ridge_model), col = 7, lty = 3)
legend("bottomleft", legend = c(colnames(X)), col = 1:700, lty = 1)
```
Now let's take a look at the coefficients of the best model we've managed to get.
```{r}
final_ridge_model <- glmnet(as.matrix(X), y, alpha=0, lambda=best_lambda)

abs_coef <- abs(coef(final_ridge_model)[-1])
```

```{r, echo=FALSE}
min(abs_coef)
paste("Number of value higher than 10^-1 : ", sum(abs_coef > 10^-1))
paste("Number of value higher than 10^-2 : ", sum(abs_coef > 10^-2))
paste("Number of value higher than 10^-3 : ", sum(abs_coef > 10^-3))
paste("Number of value higher than 10^-4 : ", sum(abs_coef > 10^-4))
```
We can see that the majority of the coefficients are lower than $10^{-1}$. Then, we could think that a lot of our co-variables are useless to predict the target variable.
(We scaled the data when doing the Ridge regression)

---

Let's do a Lasso regression to see if there are less co-variables that are actually useful to predict the fat : 

```{r, echo=FALSE}
cv_lasso_model <- cv.glmnet(as.matrix(X), y, alpha=1)
plot(cv_lasso_model, main="Lasso Regression")
print(cv_lasso_model)

best_lambda_lasso <- cv_lasso_model$lambda.min
best_model_lasso <- glmnet(as.matrix(X), y, lambda = best_lambda_lasso, alpha = 1)
```
Again, here's the regularization path :

```{r}
plot(cv_lasso_model$glmnet.fit, xvar = "lambda", main="Lasso Regression", xlim=c(-5.5, 0))
abline(h = 0, col = 6, lty = 3)
abline(v = log(best_lambda_lasso), col = 7, lty = 3)
legend("bottomleft", legend = c(colnames(X), "Zero", "Best Lambda"), col = 1:7, lty = 1)
print(log(best_lambda_lasso))
```
Now, let's see how many co-variables we have left : 

```{r, echo=FALSE}
non_zero_coefficients <- (best_model_lasso$beta[which(best_model_lasso$beta != 0)])
non_zero_spectra <- (colnames(X)[which(best_model_lasso$beta != 0)])
data.frame(non_zero_spectra, non_zero_coefficients)
```

```{r, echo=FALSE}
print(best_model_lasso)
```


We can see that our model is pretty accurate (deviance of 98.09%) with only 31 co-variables used among the 700 existant.

Actually, we've shown that even less co-variables are useless than what we thought.

Now let's try to split our dataset into train and test dataset:

**Ridge** :
```{r}
# We split into 2 dataframe randomly
indice_train <- sample(1:nrow(cookies_data), 0.8 * nrow(cookies_data))
train_data <- cookies_data[indice_train, ]
test_data <- cookies_data[-indice_train, ]

# We define X & y for both dataframe
y_train <- train_data[, 1]
X_train <- train_data[, -1]
y_test <- test_data[, 1]
X_test <- test_data[, -1]

# We train the model with the best value for lambda
cv_ridge_model <- cv.glmnet(as.matrix(X_train), y_train, alpha = 0, grouped = FALSE)
best_lambda_ridge <- cv_ridge_model$lambda.min
ridge_model <- glmnet(as.matrix(X_train), y_train, lambda = best_lambda_ridge, alpha = 0)

predictions_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = as.matrix(X_test))

error_ridge <- sqrt(mean((predictions_ridge - y_test)^2))
print(paste("RMSE Ridge :", round(error_ridge, 2)))
```


**Lasso** : 
```{r}
# We split into 2 dataframe randomly
indice_train <- sample(1:nrow(cookies_data), 0.8 * nrow(cookies_data))
train_data <- cookies_data[indice_train, ]
test_data <- cookies_data[-indice_train, ]

# We define X & y for both dataframe
y_train <- train_data[, 1]
X_train <- train_data[, -1]
y_test <- test_data[, 1]
X_test <- test_data[, -1]

# We train the model with the best value for lambda
cv_lasso_model <- cv.glmnet(as.matrix(X_train), y_train, alpha = 1, grouped = FALSE)
best_lambda <- cv_lasso_model$lambda.min
lasso_model <- glmnet(as.matrix(X_train), y_train, lambda = best_lambda, alpha = 1)

# We make prediction on the X_test
predictions_lasso <- predict(lasso_model, s = best_lambda, newx = as.matrix(X_test))

# We compute the RMSE
error_lasso  <- sqrt(mean((predictions_lasso - y_test)^2))
print(paste("RMSE :", round(error_lasso, 2)))
```

We see that the RMSE for the Lasso regression is way better than for the Ridge one.

Finally, let's verify if only 31 co-variables are useful for our prediction by using a Step forward selection :

```{r, echo=FALSE}
resall<-glm(fat~.,data=cookies_data,family=gaussian)
res0<-glm(fat~1,data=cookies_data,family=gaussian)
resfor<-step(res0,list(upper=resall),direction='forward', trace=0)
```

```{r, echo=FALSE}
print(resfor)
```

We see that we also get only 31 degrees of freedom, the same as the lasso regression.

## Conclusion

Because we had 700 covariables, which is way greater than the number of observation (32), the matrix $X^{T}X$ is not invertible and the covariables might be correlated. Therefore, we can't use the OLS method to find the best coefficients.
We had to use a Ridge regression or a Lasso regression to find the best coefficients and a correct number of covariables. We've seen that the Lasso regression was way better than the Ridge one.
This is because the Lasso regression is a method that is used to get a sparce solution, which is what we wanted.
We've also seen that only 31 co-variables were useful to predict the fat. We've also seen that the RMSE of the Lasso regression was close to 0.1, which is really good: the model is really accurate and predictive.  