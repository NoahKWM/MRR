---
title: "TP3 MRR"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-11-11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# II. Cookies Study

##Logistic regression model using features

Let's begin with processing our dataset :
```{r}
# Load the dataset
cookies_data <- read.csv(file = "cookies.csv", header = TRUE)
# Create the binary target variable YBin
YBin <- as.numeric(cookies_data$fat > median(cookies_data$fat))

# Computation (mean, standard deviation, minimum and maximum)
cookies_data$mean <- rowMeans(cookies_data[, 2:701])
cookies_data$stDev <- apply(cookies_data[, 2:701], 1, sd)
cookies_data$min <- apply(cookies_data[, 2:701], 1, min)
cookies_data$max <- apply(cookies_data[, 2:701], 1, max)

# Computation (slope)
# Function: compute_slope
# @param: spectrum_values of a cookie (here, column 2 to 701)
# @return: slope of the spectrum curve for a cookie
compute_slope <- function(spectrum_values) {
  pos <- 1:length(spectrum_values)
  lm_model <- lm(spectrum_values ~ pos)
  slope <- coef(lm_model)[2]
  return(slope)
}

cookies_data$slope <- apply(cookies_data[, 2:701], 1, compute_slope) 

# Create a data frame with features and target variable
cookies_features_data <- data.frame(
  YBin = YBin,
  mean = cookies_data$mean,
  stDev = cookies_data$stDev,
  min = cookies_data$min,
  max = cookies_data$max,
  slope = cookies_data$slope
)

head(cookies_features_data)
print(YBin)
```

```{r}
# Fit logistic regression model
model <- glm(YBin ~ ., data = cookies_features_data, family = "binomial")

# Display the summary of the model
summary(model)
```
And here are the results of the regression :
```{r}
p1 <- predict.glm(model, type = "response")
res <- as.numeric(p1 > 0.5)

print(res)
print(cookies_features_data$YBin)
```

We see, according to the p-values, that only the standard deviation seems to have an impact on the class of fat for a given cookie. However, the threshold is relatively higher than usual (0.1 here) so it might be difficult to conclude.

## Logistic regression model using the spectra

```{r}
# Load the dataset
cookies_data <- read.csv(file = "cookies.csv", header = TRUE)

# Create the binary target variable YBin
YBin <- as.numeric(cookies_data$fat > median(cookies_data$fat))

# Spectra as predictors (without fat column)
spectra_predictors <- cookies_data[, 2:701]
# Add the YBin column to the predictors
spectra_predictors$YBin <- YBin

head(spectra_predictors$YBin)
```

Let's add some penalization to the model.

Because we have a lot of covariables and few observations, we see that $p \ll n$ and so we need to select some of them to make the model simpler.

To do so, we're going to use a $l_1$-regularization :

**Lasso** :
```{r}
# Load the glmnet package
library(glmnet)

# Convert predictors and response to matrix format
X <- as.matrix(spectra_predictors[, -ncol(spectra_predictors)])  # Exclude the YBin column
y <- spectra_predictors$YBin

# Fit regularized logistic regression model (L1 penalty)
lambdas <- 10^seq(-4, -2, 0.01)
cv_model_lasso <- cv.glmnet(X, y, family = "binomial", alpha = 1, lambda = lambdas)

# Display the optimal lambda
best_lambda_lasso <- cv_model_lasso$lambda.min
cat("Optimal lambda for Lasso:", best_lambda_lasso, "\n")

plot(cv_model_lasso)
print(cv_model_lasso)
```

Here's the regulariztions path :
```{r}
plot(cv_model_lasso$glmnet.fit, xvar = "lambda", main="Lasso Regression")
abline(h = 0, col = 6, lty = 3)
abline(v = log(best_lambda_lasso), col = 7, lty = 3)
legend("bottomleft", legend = c(colnames(X), "Zero", "Best Lambda"), col = 1:7, lty = 1)
```


Since with the best $\lambda$ we get 9 not null coefficients over the 700 initial covariables, against 6 for the 1 Standard Error $\lambda$, we choose to use the $\lambda_{min}$.

Here's the Lasso regression with the best $\lambda$ :

```{r, echo=FALSE}
best_model_lasso <- glmnet(X, y, family = "binomial", alpha = 1, lambda = best_lambda_lasso)

print(best_model_lasso)
```
Let's now test its perfomance by using a K-fold :

```{r}
# We split into 2 dataframe randomly
indice_train <- sample(1:nrow(spectra_predictors), 0.8 * nrow(spectra_predictors))
train_data <- spectra_predictors[indice_train, ] # use to train the model
test_data <- spectra_predictors[-indice_train, ] # use to test the model 

# We define X & y for both dataframe
y_train <- train_data$YBin
X_train <- train_data[, -ncol(train_data)]
# ---
y_test <- test_data$YBin
X_test <- test_data[, -ncol(test_data)]

# We train the model with another cross-validation
# in order to get the best value for lambda
cv_lasso_model <- cv.glmnet(as.matrix(X_train), y_train, alpha = 1, family = "binomial", grouped = FALSE)
# We choose the best lambda
best_lambda <- cv_lasso_model$lambda.min
# Using the best lambda, we define our model
lasso_model <- glmnet(as.matrix(X_train), y_train, lambda = best_lambda, alpha = 1, family = "binomial")

# We make prediction on the X_test
predictions_lasso <- predict.glmnet(lasso_model, s = best_lambda, newx = as.matrix(X_test), type = "response")

# We compute the class for our prediction using the threshold 0.5
predictions <- as.numeric(predictions_lasso > 0.5)

# Compute the performance
performance <- mean(predictions == y_test)
print(paste("Performance globale de la validation croisÃ©e (accuracy):", round(performance, 2)))

# Confusion matrix 
print(table(test_data$YBin, predictions))
```
We can conclude that our model using Lasso regression is pretty accurate, and so with only 9 co-variables instead of 700 !