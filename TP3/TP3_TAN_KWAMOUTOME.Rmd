---
title: "TP3 MRR"
author: "Noah KWA MOUTOME - Victor TAN"
date: "2023-11-11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()); graphics.off()
```

# II. Cookies Study

## Logistic regression model using features

Let's begin with processing our dataset :
```{r}
# Load the dataset
cookies_data <- read.csv(file = "cookies.csv", header = TRUE)
# Create the binary target variable YBin
YBin <- as.numeric(cookies_data$fat > median(cookies_data$fat))

# Computation (mean, standard deviation, minimum and maximum)
cookies_data$mean <- rowMeans(cookies_data[, 2:701])
cookies_data$stDev <- apply(cookies_data[, 2:701], 1, sd)
cookies_data$min <- apply(cookies_data[, 2:701], 1, min)
cookies_data$max <- apply(cookies_data[, 2:701], 1, max)

# Computation (slope)
# Function: compute_slope
# @param: spectrum_values of a cookie (here, column 2 to 701)
# @return: slope of the spectrum curve for a cookie
compute_slope <- function(spectrum_values) {
  pos <- 1:length(spectrum_values)
  lm_model <- lm(spectrum_values ~ pos)
  slope <- coef(lm_model)[2]
  return(slope)
}

cookies_data$slope <- apply(cookies_data[, 2:701], 1, compute_slope) 

# Create a data frame with features and target variable
cookies_features_data <- data.frame(
  YBin = YBin,
  mean = cookies_data$mean,
  stDev = cookies_data$stDev,
  min = cookies_data$min,
  max = cookies_data$max,
  slope = cookies_data$slope
)

head(cookies_features_data)
print(YBin)
```

```{r}
# Fit logistic regression model
model <- glm(YBin ~ ., data = cookies_features_data, family = "binomial")

# Display the summary of the model
summary(model)
```

We see, according to the p-values, that only the standard deviation and the slope seem to have an impact on the class of fat for a given cookie, as they are around 0.05. The other features are not significant.

Let's see the confusion matrix and the accuracy of our model with a k-fold:
```{r}
n <- nrow(cookies_features_data)
ind <- sample(n)
tab <- cookies_features_data[ind, ]
K <- 4
lblock <- trunc(n/K)
predictions <- c()
true_values <- tab$YBin
for (k in (1:K)) {
  indk <- ((k-1) * lblock + 1):(k * lblock);
  tabTrain <- tab[-indk, ];
  modTrain <- glm(YBin ~ . , data = tabTrain, family = "binomial");
  testk <- predict.glm(modTrain, newdata = tab[indk, ], type = "response");
  predk <- ifelse(testk > 0.5, 1, 0);
  predictions <- c(predictions, predk);
}

table(true_values, predictions)

```

```{r, echo=FALSE}
print(paste("Accuracy:", round(mean(true_values == predictions), 2)))
print(paste("Error rate:", round(mean(true_values != predictions), 2)))
```

To conclude, we can say that our model is relatively accurate, with an accuracy close to 0.8.
It means that our logistic model can explain or predict if the fat will be higher or lower than the median of all the fat, given the previous computed features.
However, because we only have 32 observations, we can't be sure that our model is really accurate: we would need more observations to be sure of that.


Let's see the correlation between the features :
```{r}
cor(cookies_features_data[, -1])
```

We can also try to study the odd-values, but it wouldn't be relevant, as the covariables (features) seem dependent of each other.



## Logistic regression model using the spectra

```{r}
# Load the dataset
cookies_data <- read.csv(file = "cookies.csv", header = TRUE)

# Create the binary target variable YBin
YBin <- as.numeric(cookies_data$fat > median(cookies_data$fat))

# Spectra as predictors (without fat column)
spectra_predictors <- cookies_data[, 2:701]
# Add the YBin column to the predictors
spectra_predictors$YBin <- YBin

head(spectra_predictors$YBin)
```

Let's add some penalization to the model.

Because we have a lot of covariables and few observations, we see that $p \gg n$ and so we need to select some of them to make the model simpler.

To do so, we're going to use a $l_1$-regularization :

**Lasso** :
```{r}
# Load the glmnet package
library(glmnet)

# Convert predictors and response to matrix format
X <- as.matrix(spectra_predictors[, -ncol(spectra_predictors)])  # Exclude the YBin column
y <- spectra_predictors$YBin

# Fit regularized logistic regression model (L1 penalty)
lambdas <- 10^seq(-4, -1.5, 0.01)
cv_model_lasso <- cv.glmnet(X, y, family = "binomial", alpha = 1, lambda = lambdas)

# Display the optimal lambda
best_lambda_lasso <- cv_model_lasso$lambda.min
cat("Optimal lambda for Lasso:", best_lambda_lasso, "\n")
lasso_model <- glmnet(X, y, family = "binomial", alpha = 1, lambda = best_lambda_lasso)
print(lasso_model)

plot(cv_model_lasso)
print(cv_model_lasso)

non_zero_coefficients <- (lasso_model$beta[which(lasso_model$beta != 0)])
non_zero_spectra <- (colnames(X)[which(lasso_model$beta != 0)])
data.frame(non_zero_spectra, non_zero_coefficients)

```

Here's the regulariztions path :
```{r}
plot(cv_model_lasso$glmnet.fit, xvar = "lambda", main="Lasso Regression")
abline(h = 0, col = "blue", lty = 3)
abline(v = log(best_lambda_lasso), col = "red", lty = 3)
legend("bottomleft", legend = c(colnames(X), "Zero", "Best Lambda"), col = c(1:700, "blue", "red"), lty = 1)
```


Since with the best $\lambda$ we get 9 not null coefficients (including intercept) over the 700 initial covariables, (and 7 for the 1 Standard Error $\lambda$), we choose to use the $\lambda_{min}$.

Here's the Lasso regression with the best $\lambda$ :

```{r, echo=FALSE}
best_model_lasso <- glmnet(X, y, family = "binomial", alpha = 1, lambda = best_lambda_lasso)

print(best_model_lasso)
```

Let's now test its perfomance by using a K-fold :

```{r}
n <- nrow(spectra_predictors)
ind <- sample(n)
tab <- spectra_predictors[ind, ]
K <- 4
lblock <- trunc(n/K)
predictions <- c()
true_values <- tab$YBin
for (k in (1:K)) {
  indk <- ((k-1) * lblock + 1):(k * lblock);
  tabTrain <- tab[-indk, ];
  formula <- as.formula(paste("YBin ~ ", paste(non_zero_spectra, collapse = " + ")))
  modTrain <- glm(formula, data = tabTrain, family = "binomial", maxit = 10000);
  testk <- predict.glm(modTrain, newdata = tab[indk, ], type = "response");
  predk <- ifelse(testk > 0.5, 1, 0);
  predictions <- c(predictions, predk);
}

table(true_values, predictions)
```

```{r, echo=FALSE}
print(paste("Accuracy:", round(mean(predictions == true_values), 2)))
print(paste("Error rate:", round(mean(predictions != true_values), 2)))
```

We can conclude that our model using Lasso regression is pretty accurate, and so with only less co-variables instead of 700.
It means that our logistic model can explain or predict if the fat will be higher or lower than the median of all the fat, given these co-variables.
However, because we only have 32 observations, we can't be sure that our model is really accurate: we would need more observations to be sure of that.
